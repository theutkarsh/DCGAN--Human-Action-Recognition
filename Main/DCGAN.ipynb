{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Reshape\n",
    "from keras.layers.core import Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D, Convolution2D, AveragePooling2D,Conv2DTranspose,Conv2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm import tqdm\n",
    "    import pandas as pd\n",
    "\n",
    "    from keras.layers import Input\n",
    "    from keras.models import Model, Sequential\n",
    "    from keras.layers.core import Dense, Dropout\n",
    "    from keras.layers.advanced_activations import LeakyReLU\n",
    "    from keras.datasets import mnist\n",
    "    from keras.optimizers import Adam\n",
    "    from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.set_image_dim_ordering('th')  # ensure our dimension notation matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    #Adding a fully connected layer and reshape the vector\n",
    "    model.add(Dense(1024*4*4, input_dim=100, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Reshape((4,4,1024)))\n",
    "    \n",
    "     # Transposed convolution layer, from 4x4x1024 into 8x8x512 tensor\n",
    "    model.add(Conv2DTranspose(512, kernel_size = 3, strides = 2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "              \n",
    "     # Transposed convolution layer, from 8x8x512  to 16x16x256 tensor\n",
    "    model.add(Conv2DTranspose(256, kernel_size = 3, strides = 2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "     # Transposed convolution layer, from 16x16x256 to 32x32x128 tensor\n",
    "\n",
    "    model.add(Conv2DTranspose(128, kernel_size = 3, strides = 2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Transposed convolution layer, from 32x32x128 to 64x64x3  tensor\n",
    "\n",
    "    model.add(Conv2DTranspose(3, kernel_size = 3, strides = 2, padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    depth=64\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(depth, kernel_size=3, strides=2, input_shape=(64,64,3),padding='same', activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    #suggested value of 0.9 resulted in training oscillation and instability while reducing it to 0.5 helped stabilize training.\n",
    "    model.add(Conv2D(depth*2, kernel_size=3, strides=2,padding='same', activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(BatchNormalization(momentum=0.5))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(depth*4, kernel_size=3, strides=2,padding='same', activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(BatchNormalization(momentum=0.5))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(depth*8, kernel_size=3, strides=2,padding='same', activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(BatchNormalization(momentum=0.5))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    " #For the discriminator, the last convolution layer is flattened and then fed into a single sigmoid output.\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/activations.py:197: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,562,753\n",
      "Trainable params: 1,560,961\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "-- Discriminator -- \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,562,753\n",
      "Trainable params: 1,560,961\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = discriminator_model()\n",
    "print(\"-- Discriminator -- \")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 16384)             1654784   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 512)         4719104   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 16, 16, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 64, 64, 3)         3459      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 7,855,875\n",
      "Trainable params: 7,854,083\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "-- Generator -- \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 16384)             1654784   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 512)         4719104   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 16, 16, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 64, 64, 3)         3459      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 7,855,875\n",
      "Trainable params: 7,854,083\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = generator_model()\n",
    "print(\"-- Generator -- \")\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the Adam optimizer\n",
    "def get_optimizer():\n",
    "    return Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator():\n",
    "    optimizer = get_optimizer()\n",
    "    model= Sequential()\n",
    "    model.add(discriminator_model())\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator():\n",
    "    optimizer = get_optimizer()\n",
    "    model= Sequential()\n",
    "    model.add(generator_model())\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
    "    # We initially set trainable to False since we only want to train either the\n",
    "    # generator or discriminator at a time\n",
    "    discriminator.trainable = False\n",
    "    # gan input (noise) will be 100-dimensional vectors\n",
    "    gan_input = Input(shape=(random_dim,))\n",
    "    # the output of the generator (an image)\n",
    "    x = generator(gan_input)\n",
    "    # get the output of the discriminator (probability if the image is real or not)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height * shape[0], width * shape[1]), dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / width)\n",
    "        j = index % width\n",
    "        image[i * shape[0]:(i + 1) * shape[0], j * shape[1]:(j + 1) * shape[1]] = img[0, :, :]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=1, batch_size=128):\n",
    "    # Get the training and testing data\n",
    "    x_train, y_train, x_test, y_test = ////\n",
    "    # Split the training data into batches of size 128\n",
    "    batch_count = x_train.shape[0] / batch_size\n",
    "\n",
    "    # Build our GAN netowrk\n",
    "    adam = get_optimizer()\n",
    "    generator = get_generator()\n",
    "    discriminator = get_discriminator()\n",
    "    gan = get_gan_network(discriminator, random_dim, generator, adam)\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        #For each epoch, we use tqdm to make our loops show a smart progress meter.\n",
    "        for _ in tqdm(range(int(batch_count))):\n",
    "            # Get a random set of input noise and images\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "\n",
    "            # Generate fake MNIST images\n",
    "            generated_images = generator.predict(noise)\n",
    "            X = np.concatenate([image_batch, generated_images])\n",
    "\n",
    "            # Labels for generated and real data\n",
    "            y_dis = np.zeros(2*batch_size)\n",
    "            # One-sided label smoothing\n",
    "            y_dis[:batch_size] = 0.9\n",
    "\n",
    "            # Train discriminator\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "            # Train generator\n",
    "            #We take the noised input of the Generator and trick it as real data\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "        if e == 1 or e % 20 == 0:\n",
    "            plot_generated_images(e, generator)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(100, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"self.AM = Sequential()\\nself.AM.add(self.generator())\\nself.AM.add(self.discriminator())\\nself.AM.compile(loss=\\'binary_crossentropy\\', optimizer=optimizer,metrics=[\\'accuracy\\'])'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"self.AM = Sequential()\n",
    "self.AM.add(self.generator())\n",
    "self.AM.add(self.discriminator())\n",
    "self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "metrics=['accuracy'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" def build_critic(self):\\n\\n       model = Sequential()\\n\\n       model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\\n       model.add(LeakyReLU(alpha=0.2))\\n       model.add(Dropout(0.25))\\n       model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\\n       model.add(ZeroPadding2D(padding=((0,1),(0,1))))\\n       model.add(BatchNormalization(momentum=0.8))\\n       model.add(LeakyReLU(alpha=0.2))\\n       model.add(Dropout(0.25))\\n       model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\\n       model.add(BatchNormalization(momentum=0.8))\\n       model.add(LeakyReLU(alpha=0.2))\\n       model.add(Dropout(0.25))\\n       model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\\n       model.add(BatchNormalization(momentum=0.8))\\n       model.add(LeakyReLU(alpha=0.2))\\n       model.add(Dropout(0.25))\\n       model.add(Flatten())\\n       model.add(Dense(1))\\n\\n       model.summary()\\n\\n       img = Input(shape=self.img_shape)\\n       validity = model(img)\\n\\n       return Model(img, validity)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"\" def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"\" input = Input(img_shape)\n",
    "    x =Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = ZeroPadding2D(padding=((0, 1), (0, 1)))(x)\n",
    "    x = (LeakyReLU(alpha=0.2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Conv2D(256, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"real_size = (32,32,3)\n",
    "z_size = 100\n",
    "learning_rate = 0.0002\n",
    "batch_size = 128\n",
    "epochs = 25\n",
    "alpha = 0.2\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch normalization momentum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
